{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "https://github.com/pzaino/thecrowler/main/schemas/source-config-schema.json",
  "title": "CROWler Source Configuration",
  "description": "This is the configuration for a Source (base URL) that the CROWler will use to crawl websites.",
  "version": "1.0.0",
  "type": "object",
  "properties": {
    "version": {
      "title": "CROWler Source Configuration version",
      "description": "This is the version of the CROWler Source configuration, this is for you to version your work.",
      "type": "string",
      "pattern": "^\\d+\\.\\d+\\.\\d+$"
    },
    "format_version": {
      "title": "CROWler Source Configuration Schema version",
      "description": "This is the version of the CROWler Source configuration schema.",
      "type": "string",
      "pattern": "^\\d+\\.\\d+\\.\\d+$",
      "examples": [
        "1.0.0"
      ]
    },
    "author": {
      "title": "CROWler Source Configuration author",
      "description": "This is the author of the CROWler Source configuration.",
      "type": "string"
    },
    "description": {
      "title": "CROWler Source Configuration description",
      "description": "A description field for you to describe why this custom Source configuration.",
      "type": "string"
    },
    "created_at": {
      "title": "CROWler Source Configuration creation date",
      "description": "This is the date when the CROWler Source configuration was created.",
      "type": "string",
      "pattern": "(?:(?:(?:(\\d{4})[-\\/\\.](\\d{2})[-\\/\\.](\\d{2}))|(?:(\\d{2})[-\\/\\.](\\d{2})[-\\/\\.](\\d{4})))\\s*(?:T\\s*)?)?(?:(\\d{1,2}):(\\d{2})(?::(\\d{2}))?\\s*([AaPp][Mm])?)?"
    },

    "source_name": {
      "title": "CROWler Configuration source name",
      "description": "This is the name of the source (URL) that the CROWler will crawl.",
      "type": "string"
    },

    "crawling_config": {
      "type": "object",
      "properties": {
        "site": {
          "type": "string",
          "format": "uri"
        }
      },
      "required": [
        "site"
      ]
    },

    "crawler": {
      "title": "CROWler Engine's crawling Configuration",
      "description": "This is the crawler (CROWler Engine) configuration section, it's used to tell the CROWler's engine how to behave. It is the configuration for the CROWler engine that the CROWler will use to crawl websites, spin workers and configure it's internal Control API.",
      "type": "object",
      "properties": {
        "workers": {
          "title": "CROWler Engine Workers",
          "description": "This is the number of workers that the CROWler Engine will use to crawl websites. Minimum number is 3 per each Source if you have network discovery enabled or 1 per each source if you are doing crawling only. Increase the number of workers to scale up the CROWler engine vertically.",
          "type": "integer",
          "minimum": 3,
          "examples": [
            5,
            10
          ]
        },
        "interval": {
          "title": "CROWler Engine Page Rendering Interval",
          "description": "This is the interval at which the CROWler Engine will crawl websites. It is the part of the HBS, values are in seconds, e.g. '3' means 3 seconds. For the interval you can also use the CROWler exprterpreter to generate delay values at runtime, e.g., 'random(1, 3)' or 'random(random(1,3), random(5,8))'.",
          "type": "string",
          "examples": [
            "3",
            "random(1, 3)",
            "random(random(1,3), random(5,8))"
          ]
        },
        "timeout": {
          "title": "CROWler Engine Page Fetching Timeout",
          "description": "This is the timeout (in seconds) for the CROWler Engine. It is the maximum amount of time that the CROWler Engine will wait for a website to respond.",
          "type": "integer",
          "minimum": 5,
          "examples": [
            30
          ]
        },
        "maintenance": {
          "title": "CROWler Engine DB Maintenance Interval",
          "description": "This is the DB maintenance interval (in seconds) for the CROWler Engine. It is the interval at which the CROWler will perform automatic maintenance tasks, a value of 0 means NO automatic DB maintenance.",
          "type": "integer",
          "minimum": 0,
          "examples": [
            0,
            3600
          ]
        },
        "source_screenshot": {
          "title": "CROWler Engine Source Screenshot",
          "description": "This is a flag that tells the CROWler to take a screenshot of the source website. This is useful for debugging purposes.",
          "type": "boolean"
        },
        "full_site_screenshot": {
          "title": "CROWler Engine Full Site Screenshots",
          "description": "This is a flag that tells the CROWler to take a screenshot of the full website. This is useful for debugging purposes.",
          "type": "boolean"
        },
        "max_depth": {
          "title": "CROWler Engine Crawling Maximum Depth",
          "description": "This is the maximum depth that the CROWler Engine will crawl websites.",
          "type": "integer",
          "minimum": 1,
          "examples": [
            3,
            5
          ]
        },
        "max_sources": {
          "title": "CROWler Engine Maximum Sources",
          "description": "This is the maximum number of sources that a single instance of the CROWler's engine will fetch atomically and atomically to enqueue in the jobs-queue and crawl.",
          "type": "integer",
          "minimum": 1,
          "examples": [
            4,
            10,
            20
          ]
        },
        "delay": {
          "title": "CROWler Engine Delay Between Page's Fetching Requests",
          "description": "This is the delay between requests that the CROWler Engine will use to crawl websites. It is the delay between requests that the CROWler will use as part of the HBS. For delay you can also use the CROWler exprterpreter to generate delay values at runtime, e.g., 'random(1, 3)' or 'random(random(1,3), random(5,8))'.",
          "type": "string",
          "examples": [
            "3",
            "random(1, 3)",
            "random(random(1,3), random(5,8))"
          ]
        },
        "browsing_mode": {
          "title": "CROWler Engine Browsing Mode",
          "description": "This is the 'default' browsing mode that the CROWler Engine will use to crawl websites. For example, recursive, human, or fuzzing.\n- default or empty string means use recursive mode.\n- recursive means the CROWler will crawl websites in a recursive way.\n- right_click_recursive means the CROWler will crawl websites in a right-click recursive way.\n- human means the CROWler will crawl websites in a human way.\n- fuzzing means the CROWler will crawl websites by fuzzing URL and Query Parameters (this also requires crawling rules!).",
          "type": "string",
          "enum": [
            "default",
            "recursive",
            "right_click_recursive",
            "human",
            "fuzzing",
            ""
          ],
          "examples": [
            "default",
            "recursive",
            "human",
            "fuzzing"
          ]
        },
        "max_retries": {
          "title": "CROWler Engine Maximum Retries for a Website",
          "description": "This is the maximum number of times that the CROWler Engine will retry a request to a website. If the CROWler is unable to fetch a website after this number of retries, it will move on to the next website.",
          "type": "integer",
          "minimum": 0,
          "examples": [
            3,
            5
          ]
        },
        "max_requests": {
          "title": "CROWler Engine Maximum Requests for a Website",
          "description": "This is the maximum number of requests that the CROWler will send to a website. If the CROWler sends this number of requests to a website and is unable to fetch the website, it will move on to the next website. A value of 0 means no limit.",
          "type": "integer",
          "minimum": 0,
          "examples": [
            3,
            1000
          ]
        },
        "collect_html": {
          "title": "CROWler Engine Collect Page's HTML",
          "description": "This is a flag that tells the CROWler to collect the HTML of a website. This is also useful for debugging purposes. This collection is automatic and for each page of a Source.",
          "type": "boolean"
        },
        "collect_images": {
          "title": "CROWler Engine Collect Page's Images",
          "description": "This is a flag that tells the CROWler to collect images from a website. This is also useful for debugging purposes. This collection is automatic and for each page of a Source",
          "type": "boolean"
        },
        "collect_files": {
          "title": "CROWler Engine Collect Page's Files",
          "description": "This is a flag that tells the CROWler to collect files from a website. This is also useful for debugging purposes. This collection is automatic and for each page of a Source",
          "type": "boolean"
        },
        "collect_content": {
          "title": "CROWler Engine Collect Page's Content (as text)",
          "description": "This is a flag that tells the CROWler to collect the text content of a website. This is also useful for AI datasets creation and knowledge bases. This collection is automatic and for each page of a Source",
          "type": "boolean"
        },
        "collect_keywords": {
          "title": "CROWler Engine Collect Page's Keywords",
          "description": "This is a flag that tells the CROWler to collect the keywords of a website. This is also useful for AI datasets creation and knowledge bases. This collection is automatic and for each page of a Source. Keywords and metadata are used in searches, so we recommend enabling this option.",
          "type": "boolean"
        },
        "collect_metatags": {
          "title": "CROWler Engine Collect Page's Metatags",
          "description": "This is a flag that tells the CROWler to collect the metatags of a website. This is useful for AI datasets creation and knowledge bases. This collection is automatic and for each page of a Source. Keywords and metadata are used in searches, so we recommend enabling this option.",
          "type": "boolean"
        },
        "control": {
          "title": "CROWler Engine (internal) Control API Configuration",
          "description": "This is the CROWler's Control API configuration. The Control API is an internal management API that resides within the CROWler Engine. Its primary purpose is to allow internal tools, like health checks, to monitor and manage the operational status of the CROWler Engine (e.g., starting, stopping, or checking the status of crawls). It is used to control and manage engine-level operations. Important: The Control API has nothing to do with the General API (configured using the api section). The General API is an external-facing interface, exposed to interact with The CROWler to make data requests or post new sources. This section specifically configures the Control API, which operates within the CROWler Engine itself. Unlike the General API, which is designed for external interactions, the Control API is part of the CROWler’s Engine internal management system, and is not an external service.",
          "type": "object",
          "properties": {
            "host": {
              "title": "CROWler Engine Control API Host",
              "description": "This is the host that the CROWler will use to allow connections to the control API.",
              "type": "string",
              "pattern": "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
            },
            "port": {
              "title": "CROWler Engine Control API Port",
              "type": "integer",
              "minimum": 1,
              "maximum": 65535,
              "description": "This is the port that the CROWler will use to allow connections to the control API.",
              "examples": [
                8081
              ]
            },
            "timeout": {
              "title": "CROWler Engine Control API Timeout",
              "type": "integer",
              "minimum": 10,
              "description": "This is the timeout (in seconds) for the control API. It is the maximum amount of time that the CROWler will wait for the control API to respond."
            },
            "sslmode": {
              "title": "CROWler Engine Control API SSL Mode",
              "type": "string",
              "description": "This is the sslmode switch for the control API. Use 'enable' to make the control API use HTTPS.",
              "enum": [
                "enable",
                "disable",
                ""
              ],
              "examples": [
                "enable",
                "disable"
              ]
            },
            "cert_file": {
              "title": "CROWler Engine Control API Certificate File",
              "type": "string",
              "description": "This is the certificate file for the Control API HTTPS protocol."
            },
            "key_file": {
              "title": "CROWler Engine Control API Key File",
              "type": "string",
              "description": "This is the full path to the key file for the Control API HTTPS certificates."
            },
            "rate_limit": {
              "title": "CROWler Engine Control API Requests Rate Limit",
              "type": "string",
              "description": "This is the rate limit for the control API. It is the maximum number of requests that the CROWler Engine will accept per second. You can use the ExprTerpreter language to set the rate limit. The format of this parameter is 'query_per_second, total_query' (for example: '100,100').",
              "examples": [
                "100,100",
                "125000,125000"
              ]
            },
            "readheader_timeout": {
              "title": "CROWler Engine Control API Readheader Timeout",
              "type": "integer",
              "minimum": 10,
              "description": "This is the readheader timeout (in seconds) for the Control API. It is the maximum amount of time that the CROWler will wait for the control API to respond.",
              "examples": [
                30
              ]
            },
            "write_timeout": {
              "title": "CROWler Engine Control API Write Timeout",
              "type": "integer",
              "minimum": 10,
              "description": "This is the write timeout (in seconds) for the Control API. It is the maximum amount of time that the CROWler will wait for the control API to respond."
            }
          },
          "additionalProperties": false
        }
      },
      "additionalProperties": false
    },

    "selenium": {
      "title": "CROWler VDI access Configuration",
      "description": "This is the VDI configuration section, it's used to configure the VDI and tell the CROWler's Engine how to connect to it. It is the configuration for the selenium driver, to scale the CROWler web crawling capabilities, you can add multiple VDIs in an array format.",
      "type": "array",
      "items": {
        "title": "CROWler VDI Configuration Items",
        "type": "object",
        "properties": {
          "name": {
            "title": "CROWler VDI Name",
            "description": "This is the name of the VDI image. This is not a network name, so you can pick whatever makes sense for your business logic. This name can be used in a Source Configuration, to ensure the CROWler will use that specific VDI image to crawl the website.",
            "type": "string"
          },
          "location": {
            "title": "CROWler VDI Location",
            "description": "This is the location of the VDI image.",
            "type": "string"
          },
          "path": {
            "title": "CROWler Selenium Path",
            "description": "This is the path to the selenium driver (IF LOCAL). It is the path to the selenium driver that the CROWler will use to crawl websites. (deprecated)",
            "type": "string"
          },
          "driver_path": {
            "title": "CROWler Selenium Driver Path",
            "description": "This is the path to the selenium driver (IF REMOTE). It is the path to the selenium driver that the CROWler will use to crawl websites. (deprecated)",
            "type": "string"
          },
          "type": {
            "title": "CROWler VDI Browser Type",
            "description": "This is the type of selenium driver that the CROWler will use to crawl websites. For example, chrome or firefox.",
            "type": "string",
            "enum": [
              "chrome",
              "firefox",
              "chromium"
            ]
          },
          "port": {
            "title": "CROWler VDI Port",
            "description": "This is the port that the selenium driver will use to connect to the CROWler. It is the port that the selenium driver will use to connect to the CROWler.",
            "type": "integer",
            "minimum": 1,
            "maximum": 65535
          },
          "host": {
            "title": "CROWler VDI Host",
            "description": "This is the VDI host name or IP that the CROWler will use to connect to the VDI. It is the host that will be used to fetch web pages and that runs Selenium, RBee etc. For example, localhost. This is also the recommended way to use and connect to a VDI (in other words, don't try to run selenium, Rbee etc. locally, use a container for the VDI).",
            "type": "string",
            "pattern": "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
          },
          "headless": {
            "title": "CROWler VDI Headless Mode",
            "description": "This is a flag that tells the selenium driver to run in headless mode. This is useful for running the selenium driver in a headless environment. It's generally NOT recommended to enable headless mode for the selenium driver. (don't use headless unless you know what you're doing, headless browsing is mostly blocked these days!)",
            "type": "boolean"
          },
          "use_service": {
            "title": "CROWler VDI Use Service (deprecated)",
            "description": "This is a flag that tells the CROWler to access Selenium as service. (deprecated)",
            "type": "boolean"
          },
          "sslmode": {
            "title": "CROWler VDI SSL Mode",
            "description": "This is the sslmode that the selenium driver will use to connect to the CROWler. It is the sslmode that the selenium driver will use to connect to the CROWler.",
            "type": "string",
            "enum": [
              "enable",
              "disable",
              ""
            ],
            "examples": [
              "enable",
              "disable"
            ]
          },
          "download_path": {
            "title": "CROWler VDI Downloaded files Path",
            "description": "This is the temporary download path for the VDI. It is the local path where the VDI will download files. This is useful for downloading files from websites (like pdf or zip etc.). The CROWler will use this path to temporarily store the downloaded files (before moving them to the storage files area).",
            "type": "string"
          }
        },
        "additionalProperties": false,
        "required": [
          "type",
          "host"
        ]
      }
    },

    "image_storage": {
      "title": "CROWler Images Storage access Configuration",
      "description": "This is the Image Storage configuration section, it is used to tell the CROWler's Engine how and where to store collected images.",
      "type": "object",
      "properties": {
        "host": {
          "title": "CROWler Image Storage Host",
          "description": "This is the remote host for the image storage request.",
          "type": "string",
          "pattern": "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
        },
        "path": {
          "title": "CROWler Image Storage Path",
          "description": "This is the path to the image storage. It is the path to the storage that the CROWler will use to store images. if the image storage is local, this is the path to the local directory where the images will be stored. If the image storage is remote, this is the path to the remote storage where the images will be stored.",
          "type": "string"
        },
        "port": {
          "title": "CROWler Image Storage Host Port",
          "description": "This is the remote port for the image storage request.",
          "type": "integer",
          "minimum": 1,
          "maximum": 65535
        },
        "region": {
          "title": "CROWler Image Storage Region",
          "description": "This is the region for the image storage request (for example for AWS s3 buckets).",
          "type": "string"
        },
        "token": {
          "title": "CROWler Image Storage Token",
          "description": "This is the token for the image storage request for remote storage.",
          "type": "string"
        },
        "secret": {
          "title": "CROWler Image Storage Secret",
          "description": "This is the secret for the image storage request for remote storage.",
          "type": "string"
        },
        "timeout": {
          "title": "CROWler Image Storage Timeout",
          "description": "This is the remote request timeout in seconds.",
          "type": "integer",
          "minimum": 10
        },
        "type": {
          "title": "CROWler Image Storage Type",
          "description": "This is the type of storage that the CROWler will use to store images. For example, s3, http or local (local is the default type).",
          "type": "string",
          "enum": [
            "s3",
            "http",
            "local",
            ""
          ]
        },
        "sslmode": {
          "title": "CROWler Image Storage SSL Mode",
          "description": "This is the ssl mode for the image storage request for remote storage. Use enable to force https over http.",
          "type": "string",
          "enum": [
            "enable",
            "disable",
            ""
          ],
          "examples": [
            "enable",
            "disable"
          ]
        }
      },
      "additionalProperties": false,
      "required": [
        "path",
        "type"
      ]
    },

    "file_storage": {
      "title": "CROWler general Files Storage access Configuration",
      "description": "This is the File Storage configuration section, it is used to tell the CROWler's Engine how and where to store collected files.",
      "type": "object",
      "properties": {
        "host": {
          "title": "CROWler File Storage Host",
          "description": "This is the remote host for the file storage request.",
          "type": "string",
          "pattern": "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
        },
        "path": {
          "title": "CROWler File Storage Path",
          "description": "This is the path to the file storage. It is the path to the storage that the CROWler will use to store files. if the file storage is local, this is the path to the local directory where the files will be stored. If the file storage is remote, this is the path to the remote storage where the files will be stored.",
          "type": "string"
        },
        "port": {
          "title": "CROWler File Storage Host Port",
          "description": "This is the remote port for the file storage request.",
          "type": "integer",
          "minimum": 1,
          "maximum": 65535
        },
        "region": {
          "title": "CROWler File Storage Region",
          "description": "This is the region for the file storage request (for example for AWS s3 buckets).",
          "type": "string"
        },
        "token": {
          "title": "CROWler File Storage Token",
          "description": "This is the token for the file storage request for remote storage.",
          "type": "string"
        },
        "secret": {
          "title": "CROWler File Storage Secret",
          "description": "This is the secret for the file storage request for remote storage.",
          "type": "string"
        },
        "timeout": {
          "title": "CROWler File Storage Timeout",
          "description": "This is the remote request timeout in seconds.",
          "type": "integer",
          "minimum": 10
        },
        "type": {
          "title": "CROWler File Storage Type",
          "description": "This is the type of storage that the CROWler will use to store files. For example, s3, http or local (local is the default type).",
          "type": "string",
          "enum": [
            "s3",
            "http",
            "local",
            ""
          ]
        },
        "sslmode": {
          "title": "CROWler File Storage SSL Mode",
          "description": "This is the ssl mode for the file storage request for remote storage. Use enable to force https over http.",
          "type": "string",
          "enum": [
            "enable",
            "disable",
            ""
          ],
          "examples": [
            "enable",
            "disable"
          ]
        }
      },
      "additionalProperties": false,
      "required": [
        "path",
        "type"
      ]
    },

    "http_headers": {
      "title": "CROWler HTTP Headers collection Configuration",
      "description": "This is the HTTP Headers collection configuration section, it is used to tell the CROWler's Engine to collect HTTP headers and how when making requests to websites.",
      "type": "object",
      "properties": {
        "enabled": {
          "title": "CROWler HTTP Headers collection Enabled",
          "description": "This is a flag that tells the CROWler to collect HTTP headers. This is useful for detecting the headers of a website.",
          "type": "boolean"
        },
        "timeout": {
          "title": "CROWler HTTP Headers collection Timeout",
          "description": "This is the timeout for the HTTP headers collection. It is the maximum amount of time that the CROWler will wait for the HTTP headers to respond.",
          "type": "integer",
          "minimum": 5
        },
        "follow_redirects": {
          "title": "CROWler HTTP Headers collection Follow Redirects",
          "description": "This is a flag that tells the CROWler to follow redirects when collecting HTTP headers. This is useful for detecting the headers of a website.",
          "type": "boolean"
        },
        "ssl_discovery": {
          "title": "CROWler HTTP Headers collection SSL Discovery",
          "description": "This is a flag that tells the CROWler to discover SSL certificates when collecting HTTP headers. This is useful for detecting the headers of a website.",
          "type": "boolean"
        },
        "proxies": {
          "title": "CROWler HTTP Headers collection Proxies",
          "description": "This is the list of proxies that the CROWler will use to collect HTTP headers.",
          "type": "array",
          "items": {
            "title": "CROWler HTTP Headers collection Proxies Items",
            "type": "object",
            "properties": {
              "host": {
                "title": "CROWler HTTP Headers collection Proxy Host",
                "description": "This is the proxy host that the CROWler will use to collect HTTP headers. It is the proxy host that the CROWler will use to collect HTTP headers.",
                "type": "string",
                "pattern": "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
              },
              "port": {
                "title": "CROWler HTTP Headers collection Proxy Port",
                "description": "This is the proxy port that the CROWler will use to collect HTTP headers. It is the proxy port that the CROWler will use to collect HTTP headers.",
                "type": "integer",
                "minimum": 1,
                "maximum": 65535
              },
              "username": {
                "title": "CROWler HTTP Headers collection Proxy Username",
                "description": "This is the proxy username that the CROWler will use to collect HTTP headers. It is the proxy username that the CROWler will use to collect HTTP headers.",
                "type": "string"
              },
              "password": {
                "title": "CROWler HTTP Headers collection Proxy Password",
                "description": "This is the proxy password that the CROWler will use to collect HTTP headers. It is the proxy password that the CROWler will use to collect HTTP headers.",
                "type": "string"
              }
            },
            "additionalItems": false
          }
        }
      },
      "additionalProperties": false
    },

    "network_info": {
      "title": "CROWler Network Information collection Configuration",
      "description": "This is the network information collection configuration section, it is used to tell the CROWler's Engine how and what network information we want to collect for each discovered entity within the crawling of a Source. This section is also used to tell the CROWler if we want to detect network vulnerabilities, open ports, etc. Collect Whois information, DNS information, and more.",
      "type": "object",
      "properties": {
        "dns": {
          "title": "CROWler Network Information collection DNS Configuration",
          "description": "This is the configuration for the DNS data collection. It is the configuration for the DNS data collection that the CROWler will use to detect the IP address of a domain, subdomains etc.",
          "type": "object",
          "properties": {
            "enabled": {
              "title": "CROWler Network Information collection DNS Enabled",
              "description": "This is a flag that tells the CROWler to use DNS techniques. This is useful for detecting the IP address of a domain.",
              "type": "boolean"
            },
            "timeout": {
              "title": "CROWler Network Information collection DNS Timeout",
              "description": "This is the timeout for the DNS database. It is the maximum amount of time that the CROWler will wait for the DNS database to respond.",
              "type": "integer",
              "minimum": 5
            },
            "rate_limit": {
              "title": "CROWler Network Information collection DNS Rate Limit",
              "description": "This is the rate limit for the DNS database. It is the maximum number of requests that the CROWler will send to the DNS database per second. You can use the ExprTerpreter language to set the rate limit.",
              "type": "string"
            }
          },
          "additionalProperties": false,
          "required": [
            "enabled"
          ]
        },
        "whois": {
          "title": "CROWler Network Information collection Whois Configuration",
          "description": "This is the configuration for the whois data collection. It is the configuration for the whois data collection that the CROWler will use to detect the owner of a domain.",
          "type": "object",
          "properties": {
            "enabled": {
              "title": "CROWler Network Information collection Whois Enabled",
              "description": "This is a flag that tells the CROWler to use whois techniques. This is useful for detecting the owner of a domain.",
              "type": "boolean"
            },
            "timeout": {
              "title": "CROWler Network Information collection Whois Timeout",
              "description": "This is the timeout for the whois database. It is the maximum amount of time that the CROWler will wait for the whois database to respond.",
              "type": "integer",
              "minimum": 5
            },
            "rate_limit": {
              "title": "CROWler Network Information collection Whois Rate Limit",
              "description": "This is the rate limit for the whois database. It is the maximum number of requests that the CROWler will send to the whois database per second. You can use the ExprTerpreter language to set the rate limit.",
              "type": "string"
            }
          },
          "additionalProperties": false,
          "required": [
            "enabled"
          ]
        },
        "netlookup": {
          "title": "CROWler Network Information collection Netlookup Configuration",
          "description": "This is the configuration for the netlookup data collection. It is the configuration for the netlookup data collection that the CROWler will use to detect the network information of a host.",
          "type": "object",
          "properties": {
            "enabled": {
              "title": "CROWler Network Information collection Netlookup Enabled",
              "description": "This is a flag that tells the CROWler to use netlookup techniques. This is useful for detecting the network information of a host.",
              "type": "boolean"
            },
            "timeout": {
              "title": "CROWler Network Information collection Netlookup Timeout",
              "description": "This is the timeout for the netlookup database. It is the maximum amount of time that the CROWler will wait for the netlookup database to respond.",
              "type": "integer",
              "minimum": 5
            },
            "rate_limit": {
              "title": "CROWler Network Information collection Netlookup Rate Limit",
              "description": "This is the rate limit for the netlookup database. It is the maximum number of requests that the CROWler will send to the netlookup database per second. You can use the ExprTerpreter language to set the rate limit.",
              "type": "string"
            }
          },
          "additionalProperties": false,
          "required": [
            "enabled"
          ]
        },
        "geo_localization": {
          "title": "CROWler Network Information collection Geolocation Configuration",
          "description": "This is the configuration for the geolocation data collection. It is the configuration for the geolocation data collection that the CROWler will use to detect the location of a host.",
          "type": "object",
          "properties": {
            "enabled": {
              "title": "CROWler Network Information collection Geolocation Enabled",
              "description": "This is a flag that tells the CROWler to use geolocation techniques. This is useful for detecting the location of a host.",
              "type": "boolean"
            },
            "path": {
              "title": "CROWler Network Information collection Geolocation Path",
              "description": "This is the path to the geolocation database. It is the path to the database that the CROWler will use to determine the location of a host.",
              "type": "string"
            },
            "type": {
              "title": "CROWler Network Information collection Geolocation Type",
              "description": "This is the type of geolocation database that the CROWler will use. It is the type of database that the CROWler will use to determine the location of a host. For example maxmind or ip2location",
              "type": "string",
              "enum": [
                "maxmind",
                "ip2location",
                ""
              ]
            },
            "timeout": {
              "title": "CROWler Network Information collection Geolocation Timeout",
              "description": "This is the timeout for the geolocation database. It is the maximum amount of time that the CROWler will wait for the geolocation database to respond.",
              "type": "integer",
              "minimum": 5
            },
            "api_key": {
              "title": "CROWler Network Information collection Geolocation API Key",
              "description": "This is the API key for the geolocation database. It is the API key that the CROWler will use to connect to the geolocation database.",
              "type": "string"
            },
            "sslmode": {
              "title": "CROWler Network Information collection Geolocation SSL Mode",
              "description": "This is the sslmode that the CROWler will use to connect to the geolocation database.",
              "type": "string",
              "enum": [
                "enable",
                "disable",
                ""
              ],
              "examples": [
                "enable",
                "disable"
              ]
            }
          },
          "additionalProperties": false,
          "required": [
            "enabled",
            "path"
          ]
        },
        "service_scout": {
          "title": "Service Scout Configuration",
          "description": "This is the configuration for the service scout data collection. It is the configuration for the service scout data collection that the CROWler will use to detect services that are running on a host, network vulnerabilities, network software versions etc.",
          "type": "object",
          "properties": {
            "enabled": {
              "title": "Service Enabled",
              "description": "This is a flag that tells the CROWler to use service scanning techniques. This is useful for detecting services that are running on a host.",
              "type": "boolean"
            },
            "timeout": {
              "title": "Service Timeout",
              "description": "This is the timeout for the scan. It is the maximum amount of time that the CROWler will wait for a host to respond to a scan.",
              "type": "integer",
              "minimum": 5
            },
            "idle_scan": {
              "title": "Service Idle Scan Config",
              "description": "This is the configuration for the idle scan.",
              "type": "object",
              "properties": {
                "host": {
                  "title": "Idle Scan Host",
                  "description": "Host FQDN or IP address.",
                  "type": "string",
                  "pattern": "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
                },
                "port": {
                  "title": "Idle Scan Host Port",
                  "type": "integer",
                  "minimum": 1,
                  "maximum": 65535,
                  "description": "Port number."
                }
              },
              "additionalProperties": false
            },
            "ping_scan": {
              "title": "Ping Scan",
              "description": "This is a flag that tells the CROWler to use ping scanning techniques. This is useful for detecting hosts that are alive.",
              "type": "boolean"
            },
            "connect_scan": {
              "title": "Connect Scan",
              "description": "This is a flag that tells the CROWler to use connect scanning techniques. This is useful for detecting services that are running on a host.",
              "type": "boolean"
            },
            "syn_scan": {
              "title": "SYN Scan",
              "description": "This is a flag that tells the CROWler to use SYN scanning techniques. This is useful for detecting services that are running on a host.",
              "type": "boolean"
            },
            "udp_scan": {
              "title": "UDP Scan",
              "description": "This is a flag that tells the CROWler to use UDP scanning techniques. This is useful for detecting services that are running on a host.",
              "type": "boolean"
            },
            "no_dns_resolution": {
              "title": "No DNS Resolution",
              "description": "This is a flag that tells the CROWler not to resolve hostnames to IP addresses. This is useful for avoiding detection by intrusion detection systems.",
              "type": "boolean"
            },
            "service_detection": {
              "title": "Service Detection",
              "description": "This is a flag that tells the CROWler to use service detection techniques. This is useful for detecting services that are running on a host.",
              "type": "boolean"
            },
            "service_db": {
              "title": "Service DB file",
              "description": "This is the service detection database.",
              "type": "string"
            },
            "os_finger_print": {
              "title": "OS Fingerprinting",
              "description": "This is a flag that tells the CROWler to use OS fingerprinting techniques. This is useful for detecting the operating system that is running on a host.",
              "type": "boolean"
            },
            "aggressive_scan": {
              "title": "Aggressive Scan",
              "description": "This is a flag that tells the CROWler to use aggressive scanning techniques. This is useful for detecting services that are running on a host.",
              "type": "boolean"
            },
            "script_scan": {
              "title": "Script Scan",
              "description": "This is a list of nmap and crowler-scanner scripts to run. This is particularly important when a user wants to do vulnerability scanning.",
              "type": "array",
              "items": {
                "type": "string",
                "examples": [
                  "http-enum",
                  "http-headers",
                  "default",
                  "vuln"
                ]
              }
            },
            "excluded_hosts": {
              "title": "Excluded Hosts",
              "description": "This is a list of hosts to exclude from the scan. The CROWler may encounter such hosts during its crawling activities, so this field makes it easy to define a list of hosts that it should always avoid scanning.",
              "type": "array",
              "items": {
                "type": "string",
                "oneOf": [
                  {
                    "format": "ipv4"
                  },
                  {
                    "format": "ipv6"
                  },
                  {
                    "format": "hostname"
                  }
                ],
                "examples": [
                  "example.com",
                  "192.168.0.1",
                  "localhost",
                  "2001:0db8:85a3:0000:0000:8a2e:0370:7334"
                ]
              },
              "additionalProperties": false
            },
            "timing_template": {
              "title": "Timing Template",
              "description": "This allows the user to set the timing template for the scan. The timing template is a string that is passed to nmap to set the timing of the scan. DO not specify values using Tx, where x is a number. Instead, use just the number, e.g., '3'.",
              "type": "string",
              "examples": [
                "3"
              ]
            },
            "host_timeout": {
              "title": "Host Timeout",
              "description": "This is the timeout for the scan. It is the maximum amount of time that the CROWler will wait for a host to respond to a scan.",
              "type": "string"
            },
            "min_rate": {
              "title": "Minimum Rate",
              "description": "This is the minimum rate at which the CROWler will scan hosts. It is the minimum number of packets that the CROWler will send to a host per second.",
              "type": "string"
            },
            "max_retries": {
              "title": "Maximum Retries",
              "description": "This is the maximum number of times that the CROWler will retry a scan on a host. If the CROWler is unable to scan a host after this number of retries, it will move on to the next host.",
              "type": "integer"
            },
            "source_port": {
              "title": "Source Port",
              "description": "This is the source port that the CROWler will use for scanning. It is the port that the CROWler will use to send packets to hosts.",
              "type": "integer",
              "minimum": 1,
              "maximum": 65535,
              "examples": [
                80
              ]
            },
            "interface": {
              "title": "Interface",
              "description": "This is the interface that the CROWler will use for scanning. It is the network interface that the CROWler will use to send packets to hosts. Use this option with a port that is behind a VPN or a proxy for better results.",
              "type": "string",
              "examples": [
                "eth0"
              ]
            },
            "spoof_ip": {
              "title": "Spoof IP",
              "description": "This is the IP address that the CROWler will use to spoof its identity. It is the IP address that the CROWler will use to send packets to hosts. Use this option with a port that is behind a VPN or a proxy for better results.",
              "type": "string",
              "examples": [
                "192.168.0.1"
              ]
            },
            "randomize_hosts": {
              "title": "Randomize Hosts List",
              "description": "This is a flag that tells the CROWler to randomize the order in which it scans hosts. This is useful for avoiding detection by intrusion detection systems.",
              "type": "boolean"
            },
            "data_length": {
              "title": "Data Length",
              "description": "This is the length of the data that the CROWler will send to hosts. It is the length of the data that the CROWler will use to send packets to hosts. Use this option with a port that is behind a VPN or a proxy for better results.",
              "type": "integer"
            },
            "delay": {
              "title": "Delay",
              "description": "This is the delay between packets that the CROWler will use for scanning. It is the delay between packets that the CROWler will use to send packets to hosts. Use this option with a port that is behind a VPN or a proxy for better results. For the delay you can also use the CROWler exprterpreter to generate delay values at runtime, e.g., 'random(1, 3)' or 'random(random(1,3), random(5,8))'.",
              "type": "string"
            },
            "mtu_discovery": {
              "title": "MTU Discovery",
              "description": "This is a flag that tells the CROWler to use MTU discovery when scanning hosts. This is useful for avoiding detection by intrusion detection systems.",
              "type": "boolean"
            },
            "scan_flags": {
              "title": "Scan Flags",
              "description": "This is the flags that the CROWler will use for scanning. It is the flags that the CROWler will use to send packets to hosts. Use this option with a port that is behind a VPN or a proxy for better results.",
              "type": "string"
            },
            "ip_fragment": {
              "title": "IP Fragment",
              "description": "This is a flag that tells the CROWler to fragment IP packets. This is useful for avoiding detection by intrusion detection systems.",
              "type": "boolean"
            },
            "min_port_number": {
              "title": "Minimum Port Number",
              "description": "This is the minimum port number to scan (default is 1).",
              "type": "integer",
              "minimum": 1
            },
            "max_port_number": {
              "title": "Maximum Port Number",
              "description": "This is the maximum port number to scan (default is 9000).",
              "type": "integer",
              "maximum": 65535
            },
            "max_parallelism": {
              "title": "Maximum Parallelism",
              "description": "This is the maximum number of parallelism used to provide scans for a single target. Multiple targets are ALWAYS scanned in parallel.",
              "type": "integer"
            },
            "dns_servers": {
              "title": "DNS Servers",
              "description": "This is a list of custom DNS servers.",
              "type": "array",
              "items": {
                "type": "string",
                "examples": [
                  "1.1.1.1"
                ]
              },
              "additionalProperties": false
            },
            "proxies": {
              "title": "Proxies",
              "description": "List of Proxies to use to perform a scan.",
              "type": "array",
              "items": {
                "type": "string"
              },
              "additionalProperties": false
            }
          },
          "additionalProperties": false,
          "required": [
            "enabled"
          ]
        }
      },
      "additionalProperties": false
    },

    "execution_plan": {
      "title": "CROWler Source Crawling Execution Plan",
      "description": "This is the execution plan for the crawling of the source. It is the plan that the CROWler will use to crawl the source.",
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "label": {
            "title": "CROWler Execution Plan Label",
            "description": "This is the label for the execution plan. It is the label that the CROWler will use to identify the execution plan's step.",
            "type": "string"
          },
          "conditions": {
            "title": "CROWler Execution Plan Conditions",
            "description": "This is the conditions for the execution plan. These are the conditions that have to be met to trigger the Execution plan action (aka apply rulesets etc.).",
            "type": "object",
            "properties": {
              "url_patterns": {
                "type": "array",
                "items": {
                  "type": "string",
                  "format": "uri-template"
                }
              }
            },
            "additionalProperties": false,
            "required": [
              "url_patterns"
            ]
          },
          "rulesets": {
            "title": "CROWler Execution Plan Rulesets to apply for this Source",
            "description": "This is the list of rulesets that the CROWler will use to apply for the source.",
            "type": "array",
            "items": {
              "type": "string"
            }
          },
          "rule_groups": {
            "title": "CROWler Execution Plan Rule Groups to apply for this Source",
            "description": "This is the list of rule groups that the CROWler will use to apply for the source. You can use this to list specific rule groups that the CROWler will have to apply for the source.",
            "type": "array",
            "items": {
              "type": "string"
            }
          },
          "rules": {
            "title": "CROWler Execution Plan Rules to apply for this Source",
            "description": "This is the list of rules that the CROWler will use to apply for the source. You can use this to list specific rules that the CROWler will have to apply for the source.",
            "type": "array",
            "items": {
              "type": "string"
            }
          },
          "additional_conditions": {
            "title": "CROWler Execution Plan Additional Conditions",
            "description": "This is the additional conditions for the execution plan. It supports custom tags, but those have to be supported by the CROWler code otherwise they'll be ignored.",
            "type": "object",
            "additionalProperties": true
          }
        },
        "required": [
          "label",
          "conditions"
        ],
        "anyOf": [
          {
            "required": [
              "rulesets"
            ]
          },
          {
            "required": [
              "rule_groups"
            ]
          },
          {
            "required": [
              "rules"
            ]
          }
        ]
      }
    }

  },
  "required": [
    "format_version",
    "source_name",
    "crawling_config",
    "execution_plan"
  ]
}
