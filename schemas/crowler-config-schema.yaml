# TheCROWler configuration YAML Schema (draft v1.0.0)
# Copyright (c) 2022 Paolo Fabio Zaino, distributed under Apache 2.0 license
---
$schema: "http://json-schema.org/draft-07/schema#"
$id: "https://github.com/pzaino/thecrowler/main/schemas/crowler-config-schema.json"
title: "CROWler Configuration Schema"
description: "This is the configuration schema for the CROWler. This schema describes all the sections of the config.yaml configuration file and provides info on configuring a fleet of CROWler's Engines, VDIs, APIs and how they should work. Keep in mind that the 'remote' property should be used on its own, and the 'database', 'crawler', 'api', 'selenium', 'network_info' properties should be used together."
type: "object"
properties:
  version:
    title: "CROWler Configuration version"
    description: "This is the version of the CROWler configuration. This is for you to version your work."
    type: "string"
    pattern: "^\\d+\\.\\d+\\.\\d+$"
  author:
    title: "CROWler Configuration author"
    description: "This is the author of the CROWler configuration."
    type: "string"
  description:
    title: "CROWler Configuration description"
    description: "A description field."
    type: "string"
  created_at:
    title: "CROWler Configuration creation date"
    description: "This is the date when the CROWler configuration was created."
    type: "string"
    pattern: "(?:(?:(?:(\\d{4})[-\/\\.](\\d{2})[-\/\\.](\\d{2}))|(?:(\\d{2})[-\/\\.](\\d{2})[-\/\\.](\\d{4})))\\s*(?:T\\s*)?)?(?:(\\d{1,2}):(\\d{2})(?::(\\d{2}))?\\s*([AaPp][Mm])?)?"
  remote:
    title: "Remote Configuration"
    description: "This is the remote configuration section to tell the CROWler's Engine that the actual config.yaml configuration has to be fetched remotely from a distribution server. If you use this section, then do not populate the other configuration sections as they will be ignored. The CROWler will fetch its configuration from the remote server and use it to start the engine."
    type: "object"
    properties:
      host:
        title: "CROWler Configuration Distribution Server Host"
        description: "This is the host that the CROWler will use to fetch its configuration."
        type: "string"
        pattern: "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
      path:
        title: "CROWler Configuration Distribution Server Path"
        description: "This is the path that the CROWler will use to fetch its configuration."
        type: "string"
      port:
        title: "CROWler Configuration Distribution Server Port"
        description: "This is the port that the CROWler will use to fetch its configuration."
        type: "integer"
        minimum: "1"
        maximum: "65535"
        examples:
        - "80"
      region:
        title: "CROWler Configuration Distribution Server Region"
        description: "This is the region that the CROWler will use to fetch its configuration. For example in case the distribution server is on an AWS S3 bucket, you can specify the region here."
        type: "string"
      token:
        title: "CROWler Configuration Distribution Server Token"
        description: "This is the token that the CROWler will use to connect to the distribution server to fetch its configuration."
        type: "string"
      secret:
        title: "CROWler Configuration Distribution Server Secret"
        description: "This is the secret that the CROWler will use to connect to the distribution server to fetch its configuration."
        type: "string"
      timeout:
        title: "CROWler Configuration Distribution Server Timeout"
        description: "This is the timeout for the CROWler to fetch its configuration."
        type: "integer"
        minimum: "10"
      type:
        title: "CROWler Configuration Distribution Server Type"
        description: "This is the type of the distribution server that the CROWler will use to fetch its configuration. For example, s3 or http."
        type: "string"
        enum:
        - "s3"
        - "http"
        - "local"
        - ""
      sslmode:
        title: "CROWler Configuration Distribution Server SSL Mode"
        description: "This is the sslmode that the CROWler will use to connect to the distribution server to fetch its configuration."
        type: "string"
        enum:
        - "enable"
        - "disable"
        - ""
        examples:
        - "enable"
        - "disable"
    additionalProperties: "false"
    required:
    - "host"
    - "path"
    - "type"
  database:
    title: "CROWler DB Configuration"
    description: "This is the database configuration section, it's used to tell the CROWler how to connect to the database to store collected data, which type of database we use and other options to optimize the database for a specific use case."
    type: "object"
    properties:
      type:
        title: "CROWler DB Type"
        description: "This is the type of the database that the CROWler will use to store data. For example, postgres."
        type: "string"
        enum:
        - "postgres"
        - "mysql"
        - "sqlite"
      host:
        title: "CROWler DB Host"
        description: "This is the host that the CROWler will use to connect to the database."
        type: "string"
        pattern: "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
      port:
        title: "CROWler DB Port"
        description: "This is the port that the CROWler will use to connect to the database."
        type: "integer"
        minimum: "1"
        maximum: "65535"
        examples:
        - "5432"
      user:
        title: "CROWler DB User"
        description: "This is the user that the CROWler will use to connect to the database."
        type: "string"
      password:
        title: "CROWler DB Password"
        type: "string"
      dbname:
        title: "CROWler DB Name"
        description: "This is the name of the database that the CROWler will use to store data."
        type: "string"
        examples:
        - "SitesIndex"
      retry_time:
        title: "CROWler DB Retry Time"
        description: "This is the time in seconds that the CROWler will wait before retrying a database connection."
        type: "integer"
        minimum: "5"
        examples:
        - "10"
      ping_time:
        title: "CROWler DB Ping Time"
        description: "This is the time in seconds that the CROWler will wait before pinging the database to check if it is still alive."
        type: "integer"
        minimum: "5"
        examples:
        - "10"
      sslmode:
        title: "CROWler DB SSL Mode"
        description: "This is the sslmode that the CROWler will use to connect to the database. Use 'enable' to enable the ssl mode connection to the DB. (default is 'disable')."
        type: "string"
        enum:
        - "enable"
        - "disable"
        - ""
        examples:
        - "enable"
        - "disable"
      optimize_for:
        title: "CROWler DB Optimize For"
        description: "This option allows the user to optimize the database for a specific use case. For example, if the user is doing more write operations than query, then use the value 'write'. If the user is doing more query operations than write, then use the value 'query'. If unsure leave it empty."
        type: "string"
        enum:
        - "write"
        - "query"
        - "none"
        - ""
        examples:
        - "write"
        - "query"
      max_conns:
        title: "CROWler DB Max Connections"
        description: "This is the maximum number of connections that the CROWler will use to connect to the database."
        type: "integer"
        minimum: "25"
        examples:
        - "100"
      max_idle_conns:
        title: "CROWler DB Max Idle Connections"
        description: "This is the maximum number of idle connections that the CROWler will use to connect to the database. Suggestion, keep the number of idle connections to 25% / 30% of the max connections, unless you have plenty of resources."
        type: "integer"
        minimum: "25"
        examples:
        - "50"
    additionalProperties: "false"
    required:
    - "type"
    - "host"
    - "user"
    - "password"
    - "dbname"

  crawler:
    title: "CROWler Engine's crawling Configuration"
    description: "This is the crawler (CROWler Engine) configuration section, it's used to tell the CROWler's engine how to behave. It is the configuration for the CROWler engine that the CROWler will use to crawl websites, spin workers and configure it's internal Control API."
    type: "object"
    properties:
      workers:
        title: "CROWler Engine Workers"
        description: "This is the number of workers that the CROWler Engine will use to crawl websites. Minimum number is 3 per each Source if you have network discovery enabled or 1 per each source if you are doing crawling only. Increase the number of workers to scale up the CROWler engine vertically."
        type: "integer"
        minimum: "3"
        examples:
        - "5"
        - "10"
      interval:
        title: "CROWler Engine Page Rendering Interval"
        description: "This is the interval at which the CROWler Engine will crawl websites. It is the part of the HBS, values are in seconds, e.g. '3' means 3 seconds. For the interval you can also use the CROWler exprterpreter to generate delay values at runtime, e.g., 'random(1, 3)' or 'random(random(1,3), random(5,8))'."
        type: "string"
        examples:
        - "3"
        - "random(1, 3)"
        - "random(random(1,3), random(5,8))"
      timeout:
        title: "CROWler Engine Page Fetching Timeout"
        description: "This is the timeout (in seconds) for the CROWler Engine. It is the maximum amount of time that the CROWler Engine will wait for a website to respond."
        type: "integer"
        minimum: "5"
        examples:
        - "30"
      maintenance:
        title: "CROWler Engine DB Maintenance Interval"
        description: "This is the DB maintenance interval (in seconds) for the CROWler Engine. It is the interval at which the CROWler will perform automatic maintenance tasks, a value of 0 means NO automatic DB maintenance."
        type: "integer"
        minimum: "0"
        examples:
        - "0"
        - "3600"
      source_screenshot:
        title: "CROWler Engine Source Screenshot"
        description: "This is a flag that tells the CROWler to take a screenshot of the source website. This is useful for debugging purposes."
        type: "boolean"
      full_site_screenshot:
        title: "CROWler Engine Full Site Screenshots"
        description: "This is a flag that tells the CROWler to take a screenshot of the full website. This is useful for debugging purposes."
        type: "boolean"
      max_depth:
        title: "CROWler Engine Crawling Maximum Depth"
        description: "This is the maximum depth that the CROWler Engine will crawl websites."
        type: "integer"
        minimum: "1"
        examples:
        - "3"
        - "5"
      max_links:
        title: "CROWler Engine Maximum Links"
        description: "This is the maximum number of links that the CROWler Engine will crawl per Source. if zero then no limit."
        type: "integer"
        minimum: "1"
        examples:
        - "10"
        - "100"
      max_sources:
        title: "CROWler Engine Maximum Sources"
        description: "This is the maximum number of sources that a single instance of the CROWler's engine will fetch atomically and atomically to enqueue in the jobs-queue and crawl."
        type: "integer"
        minimum: "1"
        examples:
        - "4"
        - "10"
        - "20"
      delay:
        title: "CROWler Engine Delay Between Page's Fetching Requests"
        description: "This is the delay between requests that the CROWler Engine will use to crawl websites. It is the delay between requests that the CROWler will use as part of the HBS. For delay you can also use the CROWler exprterpreter to generate delay values at runtime, e.g., 'random(1, 3)' or 'random(random(1,3), random(5,8))'."
        type: "string"
        examples:
        - "3"
        - "random(1, 3)"
        - "random(random(1,3), random(5,8))"
      browsing_mode:
        title: "CROWler Engine Browsing Mode"
        description: "This is the 'default' browsing mode that the CROWler Engine will use to crawl websites. For example, recursive, human, or fuzzing.\n- default or empty string means use recursive mode.\n- recursive means the CROWler will crawl websites in a recursive way.\n- right_click_recursive means the CROWler will crawl websites in a right-click recursive way.\n- human means the CROWler will crawl websites in a human way.\n- fuzzing means the CROWler will crawl websites by fuzzing URL and Query Parameters (this also requires crawling rules!)."
        type: "string"
        enum:
        - "default"
        - "recursive"
        - "right_click_recursive"
        - "human"
        - "fuzzing"
        - ""
        examples:
        - "default"
        - "recursive"
        - "human"
        - "fuzzing"
      max_retries:
        title: "CROWler Engine Maximum Retries for a Website"
        description: "This is the maximum number of times that the CROWler Engine will retry a request to a website. If the CROWler is unable to fetch a website after this number of retries, it will move on to the next website."
        type: "integer"
        minimum: "0"
        examples:
        - "3"
        - "5"
      max_requests:
        title: "CROWler Engine Maximum Requests for a Website"
        description: "This is the maximum number of requests that the CROWler will send to a website. If the CROWler sends this number of requests to a website and is unable to fetch the website, it will move on to the next website. A value of 0 means no limit."
        type: "integer"
        minimum: "0"
        examples:
        - "3"
        - "1000"
      collect_html:
        title: "CROWler Engine Collect Page's HTML"
        description: "This is a flag that tells the CROWler to collect the HTML of a website. This is also useful for debugging purposes. This collection is automatic and for each page of a Source."
        type: "boolean"
      collect_images:
        title: "CROWler Engine Collect Page's Images"
        description: "This is a flag that tells the CROWler to collect images from a website. This is also useful for debugging purposes. This collection is automatic and for each page of a Source"
        type: "boolean"
      collect_files:
        title: "CROWler Engine Collect Page's Files"
        description: "This is a flag that tells the CROWler to collect files from a website. This is also useful for debugging purposes. This collection is automatic and for each page of a Source"
        type: "boolean"
      collect_content:
        title: "CROWler Engine Collect Page's Content (as text)"
        description: "This is a flag that tells the CROWler to collect the text content of a website. This is also useful for AI datasets creation and knowledge bases. This collection is automatic and for each page of a Source"
        type: "boolean"
      collect_keywords:
        title: "CROWler Engine Collect Page's Keywords"
        description: "This is a flag that tells the CROWler to collect the keywords of a website. This is also useful for AI datasets creation and knowledge bases. This collection is automatic and for each page of a Source. Keywords and metadata are used in searches, so we recommend enabling this option."
        type: "boolean"
      collect_metatags:
        title: "CROWler Engine Collect Page's Metatags"
        description: "This is a flag that tells the CROWler to collect the metatags of a website. This is useful for AI datasets creation and knowledge bases. This collection is automatic and for each page of a Source. Keywords and metadata are used in searches, so we recommend enabling this option."
        type: "boolean"
      collect_performance:
        title: "CROWler Engine Collect Page's Performance Metrics"
        description: "This is a flag that tells the CROWler to collect the performance metrics of each website's page."
        type: "boolean"
      collect_events:
        title: "CROWler Engine Collect Page's Events"
        description: "This is a flag that tells the CROWler to collect the events of a website. This is useful for Cybersecurity applications, given it collects every page events, included Javascript calling-back home etc."
        type: "boolean"
      control:
        title: "CROWler Engine (internal) Control API Configuration"
        description: "This is the CROWler's Control API configuration. The Control API is an internal management API that resides within the CROWler Engine. Its primary purpose is to allow internal tools, like health checks, to monitor and manage the operational status of the CROWler Engine (e.g., starting, stopping, or checking the status of crawls). It is used to control and manage engine-level operations. Important: The Control API has nothing to do with the General API (configured using the api section). The General API is an external-facing interface, exposed to interact with The CROWler to make data requests or post new sources. This section specifically configures the Control API, which operates within the CROWler Engine itself. Unlike the General API, which is designed for external interactions, the Control API is part of the CROWler’s Engine internal management system, and is not an external service."
        type: "object"
        properties:
          host:
            title: "CROWler Engine Control API Host"
            description: "This is the host that the CROWler will use to allow connections to the control API."
            type: "string"
            pattern: "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
          port:
            title: "CROWler Engine Control API Port"
            type: "integer"
            minimum: "1"
            maximum: "65535"
            description: "This is the port that the CROWler will use to allow connections to the control API."
            examples:
            - "8081"
          timeout:
            title: "CROWler Engine Control API Timeout"
            type: "integer"
            minimum: "10"
            description: "This is the timeout (in seconds) for the control API. It is the maximum amount of time that the CROWler will wait for the control API to respond."
          sslmode:
            title: "CROWler Engine Control API SSL Mode"
            type: "string"
            description: "This is the sslmode switch for the control API. Use 'enable' to make the control API use HTTPS."
            enum:
            - "enable"
            - "disable"
            - ""
            examples:
            - "enable"
            - "disable"
          cert_file:
            title: "CROWler Engine Control API Certificate File"
            type: "string"
            description: "This is the certificate file for the Control API HTTPS protocol."
          key_file:
            title: "CROWler Engine Control API Key File"
            type: "string"
            description: "This is the full path to the key file for the Control API HTTPS certificates."
          rate_limit:
            title: "CROWler Engine Control API Requests Rate Limit"
            type: "string"
            description: "This is the rate limit for the control API. It is the maximum number of requests that the CROWler Engine will accept per second. You can use the ExprTerpreter language to set the rate limit. The format of this parameter is 'query_per_second, total_query' (for example: '100,100')."
            examples:
            - "100,100"
            - "125000,125000"
          readheader_timeout:
            title: "CROWler Engine Control API Readheader Timeout"
            type: "integer"
            minimum: "10"
            description: "This is the readheader timeout (in seconds) for the Control API. It is the maximum amount of time that the CROWler will wait for the control API to respond."
            examples:
            - "30"
          write_timeout:
            title: "CROWler Engine Control API Write Timeout"
            type: "integer"
            minimum: "10"
            description: "This is the write timeout (in seconds) for the Control API. It is the maximum amount of time that the CROWler will wait for the control API to respond."
        additionalProperties: "false"
    additionalProperties: "false"

  api:
    title: "CROWler General/Search API Configuration"
    description: "This is the General/Search API configuration section, it's used to configure the General/Search API and it has no effect on the CROWler's engine, nor it has anything to do with the Engine's Control API. It is the configuration for the API that the CROWler will use to communicate with the outside world, to allow users to make queries, post Sources, check the status of the crawling activities etc."
    type: "object"
    properties:
      host:
        title: "CROWler General/Search API Host"
        description: "This is the host that the API will use to communicate with the outside world. Use 0.0.0.0 to make the API accessible from any IP address."
        type: "string"
        pattern: "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
      port:
        title: "CROWler General/Search API Port"
        description: "This is the port that the API will use to communicate with the outside world."
        type: "integer"
        minimum: "1"
        maximum: "65535"
        examples:
        - "8080"
      timeout:
        title: "CROWler General/Search API Timeout"
        description: "This is the timeout for the API. It is the maximum amount of time that the CROWler will wait for the API to respond."
        type: "integer"
        minimum: "10"
      content_search:
        title: "CROWler General/Search API Content Search"
        description: "This is a flag that tells the CROWler to search also in the text content field of a web object. This is useful for searching for every possible details of a web object, however will reduce performance quite a bit."
        type: "boolean"
      return_content:
        title: "CROWler General/Search API Return Content"
        description: "This is a flag that tells the CROWler to return the web object's text content of a page in the search results. To improve performance, you can disable this option."
        type: "boolean"
      sslmode:
        title: "CROWler General/Search API SSL Mode"
        description: "This is the sslmode switch for the API. Use 'enable' to make the API use HTTPS."
        type: "string"
        enum:
        - "enable"
        - "disable"
        - ""
        examples:
        - "enable"
        - "disable"
      cert_file:
        title: "CROWler General/Search API Certificate File"
        description: "This is the certificate file for the General/Search API HTTPS protocol."
        type: "string"
      key_file:
        title: "CROWler General/Search API Key File"
        description: "This is the key file for the General/Search API HTTPS certificates."
        type: "string"
      rate_limit:
        title: "CROWler General/Search API Requests Rate Limit"
        description: "This is the rate limit for the General/Search API. It is the maximum number of requests that the CROWler General API will accept per second. You can use the ExprTerpreter language to set the rate limit."
        type: "string"
      readheader_timeout:
        title: "CROWler General/Search API Readheader Timeout"
        type: "integer"
        minimum: "10"
        description: "This is the readheader timeout (in seconds) for the General/Search API. It is the maximum amount of time that the CROWler will wait for the General/Search API to respond."
        examples:
        - "30"
      write_timeout:
        title: "CROWler Engine General/Search API Write Timeout"
        type: "integer"
        minimum: "10"
        description: "This is the write timeout (in seconds) for the General/Search API. It is the maximum amount of time that the CROWler will wait for the control API to respond."
      enable_console:
        title: "CROWler General/Search API Enable Admin Console"
        description: "This is a flag that tells the CROWler General API to enable the 'admin console' via the API. In other words, you'll get more endpoints to manage the CROWler via the General API instead of having to use local commands to do admin tasks."
        type: "boolean"
      return_404:
        title: "CROWler General/Search API Return 404"
        description: "This is a flag that tells the CROWler to return 404 status code if a query has no results. This is mostly a secure measure to avoid leaking information about the CROWler's internal structure. If you are not exposing the General API to the public, you can disable this option."
        type: "boolean"
    additionalProperties: "false"
    required:
    - "host"
    - "timeout"

  selenium:
    title: "CROWler VDI access Configuration"
    description: "This is the VDI configuration section, it's used to configure the VDI and tell the CROWler's Engine how to connect to it. It is the configuration for the selenium driver, to scale the CROWler web crawling capabilities, you can add multiple VDIs in an array format."
    type: "array"
    items:
      title: "CROWler VDI Configuration Items"
      type: "object"
      properties:
        name:
          title: "CROWler VDI Name"
          description: "This is the name of the VDI image. This is not a network name, so you can pick whatever makes sense for your business logic. This name can be used in a Source Configuration, to ensure the CROWler will use that specific VDI image to crawl the website."
          type: "string"
        location:
          title: "CROWler VDI Location"
          description: "This is the location of the VDI image."
          type: "string"
        language:
          title: "CROWler VDI Language"
          description: "This is the language to set the VDI image to."
          type: "string"
          examples:
          - "en"
          - "it"
        path:
          title: "CROWler Selenium Path"
          description: "This is the path to the selenium driver (IF LOCAL). It is the path to the selenium driver that the CROWler will use to crawl websites. (deprecated)"
          type: "string"
        driver_path:
          title: "CROWler Selenium Driver Path"
          description: "This is the path to the selenium driver (IF REMOTE). It is the path to the selenium driver that the CROWler will use to crawl websites. (deprecated)"
          type: "string"
        type:
          title: "CROWler VDI Browser Type"
          description: "This is the type of selenium driver that the CROWler will use to crawl websites. For example, chrome or firefox."
          type: "string"
          enum:
          - "chrome"
          - "firefox"
          - "chromium"
        port:
          title: "CROWler VDI Port"
          description: "This is the port that the selenium driver will use to connect to the CROWler. It is the port that the selenium driver will use to connect to the CROWler."
          type: "integer"
          minimum: "1"
          maximum: "65535"
        host:
          title: "CROWler VDI Host"
          description: "This is the VDI host name or IP that the CROWler will use to connect to the VDI. It is the host that will be used to fetch web pages and that runs Selenium, RBee etc. For example, localhost. This is also the recommended way to use and connect to a VDI (in other words, don't try to run selenium, Rbee etc. locally, use a container for the VDI)."
          type: "string"
          pattern: "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
        headless:
          title: "CROWler VDI Headless Mode"
          description: "This is a flag that tells the selenium driver to run in headless mode. This is useful for running the selenium driver in a headless environment. It's generally NOT recommended to enable headless mode for the selenium driver. (don't use headless unless you know what you're doing, headless browsing is mostly blocked these days!)"
          type: "boolean"
        use_service:
          title: "CROWler VDI Use Service (deprecated)"
          description: "This is a flag that tells the CROWler to access Selenium as service. (deprecated)"
          type: "boolean"
        sslmode:
          title: "CROWler VDI SSL Mode"
          description: "This is the sslmode that the selenium driver will use to connect to the CROWler. It is the sslmode that the selenium driver will use to connect to the CROWler."
          type: "string"
          enum:
          - "enable"
          - "disable"
          - ""
          examples:
          - "enable"
          - "disable"
        download_path:
          title: "CROWler VDI Downloaded files Path"
          description: "This is the temporary download path for the VDI. It is the local path where the VDI will download files. This is useful for downloading files from websites (like pdf or zip etc.). The CROWler will use this path to temporarily store the downloaded files (before moving them to the storage files area)."
          type: "string"
        proxy_url:
          title: "CROWler VDI Proxy"
          description: "This is the proxy configuration for the VDI. It is the proxy that the VDI will use to connect to the internet. This is useful for bypassing firewalls or accessing websites that are blocked in your country."
          type: "string"
          examples: "http://proxy:8080"
      additionalProperties: "false"
      required:
      - "type"
      - "host"

  prometheus:
    title: "CROWler Prometheus metrics exporter Configuration"
    description: "This is the Prometheus metrics exporter configuration section, it's used to configure the Prometheus metrics exporter and tell the CROWler's Engine how to connect to it. Prometheus metrics exporter can be used to monitor the CROWler's Engine."
    type: "object"
    properties:
      enabled:
        title: "CROWler Prometheus metrics exporter Enabled"
        description: "This is a flag that tells the CROWler to enable the Prometheus metrics. This is useful for monitoring the CROWler's Engine."
        type: "boolean"
      host:
        title: "CROWler Prometheus metrics exporter Host"
        description: "This is the Prometheus Gateway host to which each CROWler engine instance will send their metrics."
        type: "string"
        pattern: "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
      port:
        title: "CROWler Prometheus metrics exporter Host Port"
        description: "This is the port of the Prometheus Gateway host to which each CROWler engine instance will send their metrics."
        type: "integer"
        minimum: "1"
        maximum: "65535"
        examples:
        - "9090"
      timeout:
        title: "CROWler Prometheus metrics exporter Timeout"
        description: "This is the timeout for the Prometheus metrics exporter. It is the maximum amount of time that the CROWler will wait for the Prometheus metrics exporter to respond."
        type: "integer"
        minimum: "10"
        examples:
        - "30"

  image_storage:
    title: "CROWler Images Storage access Configuration"
    description: "This is the Image Storage configuration section, it is used to tell the CROWler's Engine how and where to store collected images."
    type: "object"
    properties:
      host:
        title: "CROWler Image Storage Host"
        description: "This is the remote host for the image storage request."
        type: "string"
        pattern: "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
      path:
        title: "CROWler Image Storage Path"
        description: "This is the path to the image storage. It is the path to the storage that the CROWler will use to store images. if the image storage is local, this is the path to the local directory where the images will be stored. If the image storage is remote, this is the path to the remote storage where the images will be stored."
        type: "string"
      port:
        title: "CROWler Image Storage Host Port"
        description: "This is the remote port for the image storage request."
        type: "integer"
        minimum: "1"
        maximum: "65535"
      region:
        title: "CROWler Image Storage Region"
        description: "This is the region for the image storage request (for example for AWS s3 buckets)."
        type: "string"
      token:
        title: "CROWler Image Storage Token"
        description: "This is the token for the image storage request for remote storage."
        type: "string"
      secret:
        title: "CROWler Image Storage Secret"
        description: "This is the secret for the image storage request for remote storage."
        type: "string"
      timeout:
        title: "CROWler Image Storage Timeout"
        description: "This is the remote request timeout in seconds."
        type: "integer"
        minimum: "10"
      type:
        title: "CROWler Image Storage Type"
        description: "This is the type of storage that the CROWler will use to store images. For example, s3, http or local (local is the default type)."
        type: "string"
        enum:
        - "s3"
        - "http"
        - "local"
        - ""
      sslmode:
        title: "CROWler Image Storage SSL Mode"
        description: "This is the ssl mode for the image storage request for remote storage. Use enable to force https over http."
        type: "string"
        enum:
        - "enable"
        - "disable"
        - ""
        examples:
        - "enable"
        - "disable"
    additionalProperties: "false"
    required:
    - "path"
    - "type"
  file_storage:
    title: "CROWler general Files Storage access Configuration"
    description: "This is the File Storage configuration section, it is used to tell the CROWler's Engine how and where to store collected files."
    type: "object"
    properties:
      host:
        title: "CROWler File Storage Host"
        description: "This is the remote host for the file storage request."
        type: "string"
        pattern: "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
      path:
        title: "CROWler File Storage Path"
        description: "This is the path to the file storage. It is the path to the storage that the CROWler will use to store files. if the file storage is local, this is the path to the local directory where the files will be stored. If the file storage is remote, this is the path to the remote storage where the files will be stored."
        type: "string"
      port:
        title: "CROWler File Storage Host Port"
        description: "This is the remote port for the file storage request."
        type: "integer"
        minimum: "1"
        maximum: "65535"
      region:
        title: "CROWler File Storage Region"
        description: "This is the region for the file storage request (for example for AWS s3 buckets)."
        type: "string"
      token:
        title: "CROWler File Storage Token"
        description: "This is the token for the file storage request for remote storage."
        type: "string"
      secret:
        title: "CROWler File Storage Secret"
        description: "This is the secret for the file storage request for remote storage."
        type: "string"
      timeout:
        title: "CROWler File Storage Timeout"
        description: "This is the remote request timeout in seconds."
        type: "integer"
        minimum: "10"
      type:
        title: "CROWler File Storage Type"
        description: "This is the type of storage that the CROWler will use to store files. For example, s3, http or local (local is the default type)."
        type: "string"
        enum:
        - "s3"
        - "http"
        - "local"
        - ""
      sslmode:
        title: "CROWler File Storage SSL Mode"
        description: "This is the ssl mode for the file storage request for remote storage. Use enable to force https over http."
        type: "string"
        enum:
        - "enable"
        - "disable"
        - ""
        examples:
        - "enable"
        - "disable"
    additionalProperties: "false"
    required:
    - "path"
    - "type"
  http_headers:
    title: "CROWler HTTP Headers collection Configuration"
    description: "This is the HTTP Headers collection configuration section, it is used to tell the CROWler's Engine to collect HTTP headers and how when making requests to websites."
    type: "object"
    properties:
      enabled:
        title: "CROWler HTTP Headers collection Enabled"
        description: "This is a flag that tells the CROWler to collect HTTP headers. This is useful for detecting the headers of a website."
        type: "boolean"
      timeout:
        title: "CROWler HTTP Headers collection Timeout"
        description: "This is the timeout for the HTTP headers collection. It is the maximum amount of time that the CROWler will wait for the HTTP headers to respond."
        type: "integer"
        minimum: "5"
      follow_redirects:
        title: "CROWler HTTP Headers collection Follow Redirects"
        description: "This is a flag that tells the CROWler to follow redirects when collecting HTTP headers. This is useful for detecting the headers of a website."
        type: "boolean"
      ssl_discovery:
        title: "CROWler HTTP Headers collection SSL Discovery"
        description: "This is a flag that tells the CROWler to discover SSL certificates when collecting HTTP headers. This is useful for detecting the headers of a website."
        type: "boolean"
      proxies:
        title: "CROWler HTTP Headers collection Proxies"
        description: "This is the list of proxies that the CROWler will use to collect HTTP headers."
        type: "array"
        items:
          title: "CROWler HTTP Headers collection Proxies Items"
          type: "object"
          properties:
            host:
              title: "CROWler HTTP Headers collection Proxy Host"
              description: "This is the proxy host that the CROWler will use to collect HTTP headers. It is the proxy host that the CROWler will use to collect HTTP headers."
              type: "string"
              pattern: "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
            port:
              title: "CROWler HTTP Headers collection Proxy Port"
              description: "This is the proxy port that the CROWler will use to collect HTTP headers. It is the proxy port that the CROWler will use to collect HTTP headers."
              type: "integer"
              minimum: "1"
              maximum: "65535"
            username:
              title: "CROWler HTTP Headers collection Proxy Username"
              description: "This is the proxy username that the CROWler will use to collect HTTP headers. It is the proxy username that the CROWler will use to collect HTTP headers."
              type: "string"
            password:
              title: "CROWler HTTP Headers collection Proxy Password"
              description: "This is the proxy password that the CROWler will use to collect HTTP headers. It is the proxy password that the CROWler will use to collect HTTP headers."
              type: "string"
          additionalItems: "false"
    additionalProperties: "false"
  network_info:
    title: "CROWler Network Information collection Configuration"
    description: "This is the network information collection configuration section, it is used to tell the CROWler's Engine how and what network information we want to collect for each discovered entity within the crawling of a Source. This section is also used to tell the CROWler if we want to detect network vulnerabilities, open ports, etc. Collect Whois information, DNS information, and more."
    type: "object"
    properties:
      dns:
        title: "CROWler Network Information collection DNS Configuration"
        description: "This is the configuration for the DNS data collection. It is the configuration for the DNS data collection that the CROWler will use to detect the IP address of a domain, subdomains etc."
        type: "object"
        properties:
          enabled:
            title: "CROWler Network Information collection DNS Enabled"
            description: "This is a flag that tells the CROWler to use DNS techniques. This is useful for detecting the IP address of a domain."
            type: "boolean"
          timeout:
            title: "CROWler Network Information collection DNS Timeout"
            description: "This is the timeout for the DNS database. It is the maximum amount of time that the CROWler will wait for the DNS database to respond."
            type: "integer"
            minimum: "5"
          rate_limit:
            title: "CROWler Network Information collection DNS Rate Limit"
            description: "This is the rate limit for the DNS database. It is the maximum number of requests that the CROWler will send to the DNS database per second. You can use the ExprTerpreter language to set the rate limit."
            type: "string"
        additionalProperties: "false"
        required:
        - "enabled"
      whois:
        title: "CROWler Network Information collection Whois Configuration"
        description: "This is the configuration for the whois data collection. It is the configuration for the whois data collection that the CROWler will use to detect the owner of a domain."
        type: "object"
        properties:
          enabled:
            title: "CROWler Network Information collection Whois Enabled"
            description: "This is a flag that tells the CROWler to use whois techniques. This is useful for detecting the owner of a domain."
            type: "boolean"
          timeout:
            title: "CROWler Network Information collection Whois Timeout"
            description: "This is the timeout for the whois database. It is the maximum amount of time that the CROWler will wait for the whois database to respond."
            type: "integer"
            minimum: "5"
          rate_limit:
            title: "CROWler Network Information collection Whois Rate Limit"
            description: "This is the rate limit for the whois database. It is the maximum number of requests that the CROWler will send to the whois database per second. You can use the ExprTerpreter language to set the rate limit."
            type: "string"
        additionalProperties: "false"
        required:
        - "enabled"
      netlookup:
        title: "CROWler Network Information collection Netlookup Configuration"
        description: "This is the configuration for the netlookup data collection. It is the configuration for the netlookup data collection that the CROWler will use to detect the network information of a host."
        type: "object"
        properties:
          enabled:
            title: "CROWler Network Information collection Netlookup Enabled"
            description: "This is a flag that tells the CROWler to use netlookup techniques. This is useful for detecting the network information of a host."
            type: "boolean"
          timeout:
            title: "CROWler Network Information collection Netlookup Timeout"
            description: "This is the timeout for the netlookup database. It is the maximum amount of time that the CROWler will wait for the netlookup database to respond."
            type: "integer"
            minimum: "5"
          rate_limit:
            title: "CROWler Network Information collection Netlookup Rate Limit"
            description: "This is the rate limit for the netlookup database. It is the maximum number of requests that the CROWler will send to the netlookup database per second. You can use the ExprTerpreter language to set the rate limit."
            type: "string"
        additionalProperties: "false"
        required:
        - "enabled"
      geo_localization:
        title: "CROWler Network Information collection Geolocation Configuration"
        description: "This is the configuration for the geolocation data collection. It is the configuration for the geolocation data collection that the CROWler will use to detect the location of a host."
        type: "object"
        properties:
          enabled:
            title: "CROWler Network Information collection Geolocation Enabled"
            description: "This is a flag that tells the CROWler to use geolocation techniques. This is useful for detecting the location of a host."
            type: "boolean"
          path:
            title: "CROWler Network Information collection Geolocation Path"
            description: "This is the path to the geolocation database. It is the path to the database that the CROWler will use to determine the location of a host."
            type: "string"
          type:
            title: "CROWler Network Information collection Geolocation Type"
            description: "This is the type of geolocation database that the CROWler will use. It is the type of database that the CROWler will use to determine the location of a host. For example maxmind or ip2location"
            type: "string"
            enum:
            - "maxmind"
            - "ip2location"
            - ""
          timeout:
            title: "CROWler Network Information collection Geolocation Timeout"
            description: "This is the timeout for the geolocation database. It is the maximum amount of time that the CROWler will wait for the geolocation database to respond."
            type: "integer"
            minimum: "5"
          api_key:
            title: "CROWler Network Information collection Geolocation API Key"
            description: "This is the API key for the geolocation database. It is the API key that the CROWler will use to connect to the geolocation database."
            type: "string"
          sslmode:
            title: "CROWler Network Information collection Geolocation SSL Mode"
            description: "This is the sslmode that the CROWler will use to connect to the geolocation database."
            type: "string"
            enum:
            - "enable"
            - "disable"
            - ""
            examples:
            - "enable"
            - "disable"
        additionalProperties: "false"
        required:
        - "enabled"
        - "path"
      service_scout:
        title: "Service Scout Configuration"
        description: "This is the configuration for the service scout data collection. It is the configuration for the service scout data collection that the CROWler will use to detect services that are running on a host, network vulnerabilities, network software versions etc."
        type: "object"
        properties:
          enabled:
            title: "Service Enabled"
            description: "This is a flag that tells the CROWler to use service scanning techniques. This is useful for detecting services that are running on a host."
            type: "boolean"
          timeout:
            title: "Service Timeout"
            description: "This is the timeout for the scan. It is the maximum amount of time that the CROWler will wait for a host to respond to a scan."
            type: "integer"
            minimum: "5"
          idle_scan:
            title: "Service Idle Scan Config"
            description: "This is the configuration for the idle scan."
            type: "object"
            properties:
              host:
                title: "Idle Scan Host"
                description: "Host FQDN or IP address."
                type: "string"
                pattern: "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
              port:
                title: "Idle Scan Host Port"
                type: "integer"
                minimum: "1"
                maximum: "65535"
                description: "Port number."
            additionalProperties: "false"
          ping_scan:
            title: "Ping Scan"
            description: "This is a flag that tells the CROWler to use ping scanning techniques. This is useful for detecting hosts that are alive."
            type: "boolean"
          connect_scan:
            title: "Connect Scan"
            description: "This is a flag that tells the CROWler to use connect scanning techniques. This is useful for detecting services that are running on a host."
            type: "boolean"
          syn_scan:
            title: "SYN Scan"
            description: "This is a flag that tells the CROWler to use SYN scanning techniques. This is useful for detecting services that are running on a host."
            type: "boolean"
          udp_scan:
            title: "UDP Scan"
            description: "This is a flag that tells the CROWler to use UDP scanning techniques. This is useful for detecting services that are running on a host."
            type: "boolean"
          no_dns_resolution:
            title: "No DNS Resolution"
            description: "This is a flag that tells the CROWler not to resolve hostnames to IP addresses. This is useful for avoiding detection by intrusion detection systems."
            type: "boolean"
          service_detection:
            title: "Service Detection"
            description: "This is a flag that tells the CROWler to use service detection techniques. This is useful for detecting services that are running on a host."
            type: "boolean"
          service_db:
            title: "Service DB file"
            description: "This is the service detection database."
            type: "string"
          os_finger_print:
            title: "OS Fingerprinting"
            description: "This is a flag that tells the CROWler to use OS fingerprinting techniques. This is useful for detecting the operating system that is running on a host."
            type: "boolean"
          aggressive_scan:
            title: "Aggressive Scan"
            description: "This is a flag that tells the CROWler to use aggressive scanning techniques. This is useful for detecting services that are running on a host."
            type: "boolean"
          script_scan:
            title: "Script Scan"
            description: "This is a list of nmap and crowler-scanner scripts to run. This is particularly important when a user wants to do vulnerability scanning."
            type: "array"
            items:
              type: "string"
              examples:
              - "http-enum"
              - "http-headers"
              - "default"
              - "vuln"
          excluded_hosts:
            title: "Excluded Hosts"
            description: "This is a list of hosts to exclude from the scan. The CROWler may encounter such hosts during its crawling activities, so this field makes it easy to define a list of hosts that it should always avoid scanning."
            type: "array"
            items:
              type: "string"
              oneOf:
              - format: "ipv4"
              - format: "ipv6"
              - format: "hostname"
              examples:
              - "example.com"
              - "192.168.0.1"
              - "localhost"
              - "2001:0db8:85a3:0000:0000:8a2e:0370:7334"
            additionalProperties: "false"
          timing_template:
            title: "Timing Template"
            description: "This allows the user to set the timing template for the scan. The timing template is a string that is passed to nmap to set the timing of the scan. DO not specify values using Tx, where x is a number. Instead, use just the number, e.g., '3'."
            type: "string"
            examples:
            - "3"
          host_timeout:
            title: "Host Timeout"
            description: "This is the timeout for the scan. It is the maximum amount of time that the CROWler will wait for a host to respond to a scan."
            type: "string"
          min_rate:
            title: "Minimum Rate"
            description: "This is the minimum rate at which the CROWler will scan hosts. It is the minimum number of packets that the CROWler will send to a host per second."
            type: "string"
          max_retries:
            title: "Maximum Retries"
            description: "This is the maximum number of times that the CROWler will retry a scan on a host. If the CROWler is unable to scan a host after this number of retries, it will move on to the next host."
            type: "integer"
          source_port:
            title: "Source Port"
            description: "This is the source port that the CROWler will use for scanning. It is the port that the CROWler will use to send packets to hosts."
            type: "integer"
            minimum: "1"
            maximum: "65535"
            examples:
            - "80"
          interface:
            title: "Interface"
            description: "This is the interface that the CROWler will use for scanning. It is the network interface that the CROWler will use to send packets to hosts. Use this option with a port that is behind a VPN or a proxy for better results."
            type: "string"
            examples:
            - "eth0"
          spoof_ip:
            title: "Spoof IP"
            description: "This is the IP address that the CROWler will use to spoof its identity. It is the IP address that the CROWler will use to send packets to hosts. Use this option with a port that is behind a VPN or a proxy for better results."
            type: "string"
            examples:
            - "192.168.0.1"
          randomize_hosts:
            title: "Randomize Hosts List"
            description: "This is a flag that tells the CROWler to randomize the order in which it scans hosts. This is useful for avoiding detection by intrusion detection systems."
            type: "boolean"
          data_length:
            title: "Data Length"
            description: "This is the length of the data that the CROWler will send to hosts. It is the length of the data that the CROWler will use to send packets to hosts. Use this option with a port that is behind a VPN or a proxy for better results."
            type: "integer"
          delay:
            title: "Delay"
            description: "This is the delay between packets that the CROWler will use for scanning. It is the delay between packets that the CROWler will use to send packets to hosts. Use this option with a port that is behind a VPN or a proxy for better results. For the delay you can also use the CROWler exprterpreter to generate delay values at runtime, e.g., 'random(1, 3)' or 'random(random(1,3), random(5,8))'."
            type: "string"
          mtu_discovery:
            title: "MTU Discovery"
            description: "This is a flag that tells the CROWler to use MTU discovery when scanning hosts. This is useful for avoiding detection by intrusion detection systems."
            type: "boolean"
          scan_flags:
            title: "Scan Flags"
            description: "This is the flags that the CROWler will use for scanning. It is the flags that the CROWler will use to send packets to hosts. Use this option with a port that is behind a VPN or a proxy for better results."
            type: "string"
          ip_fragment:
            title: "IP Fragment"
            description: "This is a flag that tells the CROWler to fragment IP packets. This is useful for avoiding detection by intrusion detection systems."
            type: "boolean"
          min_port_number:
            title: "Minimum Port Number"
            description: "This is the minimum port number to scan (default is 1)."
            type: "integer"
            minimum: "1"
          max_port_number:
            title: "Maximum Port Number"
            description: "This is the maximum port number to scan (default is 9000)."
            type: "integer"
            maximum: "65535"
          max_parallelism:
            title: "Maximum Parallelism"
            description: "This is the maximum number of parallelism used to provide scans for a single target. Multiple targets are ALWAYS scanned in parallel."
            type: "integer"
          dns_servers:
            title: "DNS Servers"
            description: "This is a list of custom DNS servers."
            type: "array"
            items:
              type: "string"
              examples:
              - "1.1.1.1"
            additionalProperties: "false"
          proxies:
            title: "Proxies"
            description: "List of Proxies to use to perform a scan."
            type: "array"
            items:
              type: "string"
            additionalProperties: "false"
        additionalProperties: "false"
        required:
        - "enabled"
    additionalProperties: "false"
  rulesets_schema_path:
    title: "CROWler Ruleset JSON Schema location Configuration"
    description: "This is the path to the rulesets schema. It is the path to the schema that the CROWler will use to validate the rulesets."
    type: "string"
    examples:
    - "./schemas/rulesets.schema.json"
  rulesets:
    title: "CROWler Rulesets locations Configuration"
    description: "This is the rulesets load configuration section, it is used to tell the CROWler where and how to load all the Rulesets we want to use to crawl, interact, scrape info and detect stuff on the provided Sources to crawl."
    type: "array"
    items:
      title: "CROWler Rulesets locations Configuration parameters"
      type: "object"
      properties:
        path:
          title: "CROWler Rulesets Path"
          description: "This is the path that the CROWler will use to fetch the ruleset. You can use wildcard to fetch multiple rulesets. for example './rules/*.yaml'."
          type: "array"
          items:
            type: "string"
          examples:
          -
          - "./rules/*.yaml"
        host:
          title: "CROWler Rulesets Host"
          description: "This is the host that the CROWler will use to fetch the ruleset."
          type: "string"
          pattern: "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
        port:
          title: "CROWler Rulesets Host Port"
          description: "This is the port that the CROWler will use to fetch the ruleset."
          type: "integer"
          minimum: "1"
          maximum: "65535"
          examples:
          - "443"
        region:
          title: "CROWler Rulesets Host Region"
          description: "This is the region that the CROWler will use to fetch the ruleset (for example, if you're hosting your ruleset on an AWS S3 bucket)."
          type: "string"
        token:
          title: "CROWler Rulesets Host access Token"
          description: "This is the token that the CROWler will use to connect to the distribution server to download the ruleset."
          type: "string"
        secret:
          title: "CROWler Rulesets Host access Secret"
          description: "This is the secret that the CROWler will use to connect to the distribution server to download the ruleset."
          type: "string"
        timeout:
          title: "CROWler Rulesets Host Timeout"
          description: "This is the timeout in seconds for the CROWler to fetch the ruleset."
          type: "integer"
          minimum: "10"
        type:
          title: "CROWler Rulesets Host Type"
          description: "This is the type of the distribution server that the CROWler will use to fetch the ruleset. For example, s3, http or local. (local is default)"
          type: "string"
          enum:
          - "s3"
          - "http"
          - "local"
          - ""
        sslmode:
          title: "CROWler Rulesets Host SSL Mode"
          description: "This is the sslmode that the CROWler will use to connect to the distribution server to fetch the ruleset. Use 'enable' to force https over http."
          type: "string"
          enum:
          - "enable"
          - "disable"
          - ""
          examples:
          - "enable"
          - "disable"
        refresh:
          title: "CROWler Rulesets Automatic Refresh Interval"
          description: "This is the refresh interval in seconds for the CROWler to fetch the ruleset (refresh it)."
          type: "integer"
          minimum: "0"
      additionalProperties: "false"
      required:
      - "path"
      - "type"
    minItems: "1"
    uniqueItems: "true"
  plugins:
    title: "CROWler Plugins locations Configuration"
    description: "This is the Plugins configuration section, it is used to tell the CROWler's Engine which plugins we want to use and provide the path to the plugins. Plugins are used to extend the CROWler's capabilities, for example, to detect phishing URLs, detect malware, detect network vulnerabilities, etc."
    type: "object"
    properties:
      timeout:
        title: "CROWler Plugin Execution Timeout"
        description: "This is the timeout in seconds for the CROWler to execute a plugin."
        type: "integer"
        minimum: "10"
      locations:
        title: "CROWler Plugins Locations List"
        description: "This is the list of locations from where the CROWler will load and register all available plugins."
        type: "array"
        items:
          title: "CROWler Plugins Location Configuration"
          type: "object"
          properties:
            path:
              title: "CROWler Plugins location Path"
              description: "This is the path to the plugin."
              type: "array"
              items:
                type: "string"
                description: "This is a list of paths from where the CROWler will search for plugins to load."
                examples:
                - "./plugins/"
            host:
              title: "CROWler Plugins location Host"
              description: "This is the host that the CROWler will use to fetch the plugin."
              type: "string"
              pattern: "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
            port:
              title: "CROWler Plugins location Host Port"
              description: "This is the port that the CROWler will use to fetch the plugin."
              type: "integer"
              minimum: "1"
              maximum: "65535"
            region:
              title: "CROWler Plugins location Host Region"
              description: "This is the region that the CROWler will use to fetch the plugin (for example, if you're hosting your plugin on an AWS S3 bucket)."
              type: "string"
            token:
              title: "CROWler Plugins location Host access Token"
              description: "This is the token that the CROWler will use to connect to the distribution server to download the plugin."
              type: "string"
            secret:
              title: "CROWler Plugins location Host access Secret"
              description: "This is the secret that the CROWler will use to connect to the distribution server to download the plugin."
              type: "string"
            timeout:
              title: "CROWler Plugins location Host Timeout"
              description: "This is the timeout in seconds for the CROWler to fetch the plugin."
              type: "integer"
              minimum: "10"
            type:
              title: "CROWler Plugins location Host Type"
              description: "This is the type of the distribution server that the CROWler will use to fetch the plugin. For example, s3, http or local. (local is default)"
              type: "string"
              enum:
              - "s3"
              - "http"
              - "local"
              - ""
            sslmode:
              title: "CROWler Plugins location Host SSL Mode"
              description: "This is the sslmode that the CROWler will use to connect to the distribution server to fetch the plugin. Use 'enable' to force https over http."
              type: "string"
              enum:
              - "enable"
              - "disable"
              - ""
              examples:
              - "enable"
              - "disable"
            refresh:
              title: "CROWler Plugins location Automatic Refresh Interval"
              description: "This is the refresh interval in seconds for the CROWler to fetch the plugin (refresh it). Zero means no refresh."
              type: "integer"
              minimum: "0"
          additionalProperties: "false"
  external_detection:
    title: "CROWler External Detection Services Configuration"
    description: "This is the External Detection configuration section, it is used to tell the CROWler's Engine which external detection services we want to use and provide credentials to access them. External detection services are provided by VirusTotal, URLHaus, PhishTank, GoogleSafeBrowsing, AbuseIPDB, OpenPhish, Cuckoo, HybridAnalysis, CiscoUmbrella, AlienVault, IPVoid, Shodan, Censys, SSLLabs. They can be accessed using the CROWler's Detection Rules."
    type: "object"
    properties:
      abuse_ipdb:
        title: "AbuseIPDB Configuration"
        description: "This is the AbuseIPDB configuration section, it is used to tell the CROWler's Engine how to connect to AbuseIPDB and use its services. AbuseIPDB will be accessed through CROWler's Detection Rules."
        type: "object"
        properties:
          api_key:
            title: "AbuseIPDB API Key"
            description: "This is the API key that the CROWler will use to connect to AbuseIPDB."
            type: "string"
        additionalProperties: "false"
        required:
        - "api_key"
      alien_vault:
        title: "Alien Vault Configuration"
        description: "This is the Alien Vault configuration section, it is used to tell the CROWler's Engine how to connect to Alien Vault and use its services. Alien Vault will be accessed through CROWler's Detection Rules."
        type: "object"
        properties:
          api_key:
            title: "Alien Vault API Key"
            description: "This is the API key that the CROWler will use to connect to Alien Vault."
            type: "string"
        additionalProperties: "false"
        required:
        - "api_key"
      censys:
        title: "Censys Configuration"
        description: "This is the Censys configuration section, it is used to tell the CROWler's Engine how to connect to Censys and use its services. Censys will be accessed through CROWler's Detection Rules."
        type: "object"
        properties:
          api_id:
            title: "Censys API ID"
            description: "This is the API ID that the CROWler will use to connect to Censys."
            type: "string"
          api_secret:
            title: "Censys API Secret"
            description: "This is the API secret that the CROWler will use to connect to Censys."
            type: "string"
        additionalProperties: "false"
        required:
        - "api_id"
        - "api_secret"
      cisco_umbrella:
        title: "Cisco Umbrella Configuration"
        description: "This is the Cisco Umbrella configuration section, it is used to tell the CROWler's Engine how to connect to Cisco Umbrella and use its services. Cisco Umbrella will be accessed through CROWler's Detection Rules."
        type: "object"
        properties:
          api_key:
            title: "Cisco Umbrella API Key"
            description: "This is the API key that the CROWler will use to connect to Cisco Umbrella."
            type: "string"
        additionalProperties: "false"
        required:
        - "api_key"
      cuckoo:
        title: "Cuckoo Configuration"
        description: "This is the Cuckoo configuration section, it is used to tell the CROWler's Engine how to connect to Cuckoo and use its services. Cuckoo will be accessed through CROWler's Detection Rules."
        type: "object"
        properties:
        additionalProperties: "false"
      google_safe_browsing:
        title: "Google Safe Browsing Configuration"
        description: "This is the Google Safe Browsing configuration section, it is used to tell the CROWler's Engine how to connect to Google Safe Browsing and use its services. Google Safe Browsing will be accessed through CROWler's Detection Rules."
        type: "object"
        properties:
          company_id:
            title: "Google Safe Browsing Company ID"
            description: "This is the company ID that the CROWler will use to connect to Google Safe Browsing."
            type: "string"
          api_key:
            title: "Google Safe Browsing API Key"
            description: "This is the API key that the CROWler will use to connect to Google Safe Browsing."
            type: "string"
        additionalProperties: "false"
        required:
        - "api_key"
      hybrid_analysis:
        title: "Hybrid Analysis Configuration"
        description: "This is the Hybrid Analysis configuration section, it is used to tell the CROWler's Engine how to connect to Hybrid Analysis and use its services. Hybrid Analysis will be accessed through CROWler's Detection Rules."
        type: "object"
        properties:
          api_key:
            title: "Hybrid Analysis API Key"
            description: "This is the API key that the CROWler will use to connect to Hybrid Analysis."
            type: "string"
        additionalProperties: "false"
        required:
        - "api_key"
      ipvoid:
        title: "IPVoid Configuration"
        description: "This is the IPVoid configuration section, it is used to tell the CROWler's Engine how to connect to IPVoid and use its services. IPVoid will be accessed through CROWler's Detection Rules."
        type: "object"
        properties:
          api_key:
            title: "IPVoid API Key"
            description: "This is the API key that the CROWler will use to connect to IPVoid."
            type: "string"
        additionalProperties: "false"
        required:
        - "api_key"
      open_phish:
        title: "OpenPhish Configuration"
        description: "This is the OpenPhish configuration section, it is used to tell the CROWler's Engine how to connect to OpenPhish and use its services. OpenPhish will be accessed through CROWler's Detection Rules."
        type: "object"
        properties:
          api_key:
            title: "OpenPhish API Key"
            description: "This is the API key that the CROWler will use to connect to OpenPhish."
            type: "string"
        additionalProperties: "false"
        required:
        - "api_key"
      phish_tank:
        title: "PhishTank Configuration"
        description: "This is the PhishTank configuration section, it is used to tell the CROWler's Engine how to connect to PhishTank and use its services. PhishTank will be accessed through CROWler's Detection Rules."
        type: "object"
        properties:
        additionalProperties: "false"
      shodan:
        title: "Shodan Configuration"
        description: "This is the Shodan configuration section, it is used to tell the CROWler's Engine how to connect to Shodan and use its services. Shodan will be accessed through CROWler's Detection Rules."
        type: "object"
        properties:
          api_key:
            title: "Shodan API Key"
            description: "This is the API key that the CROWler will use to connect to Shodan."
            type: "string"
        additionalProperties: "false"
        required:
        - "api_key"
      ssllabs:
        title: "SSLLabs Configuration"
        description: "This is the SSLLabs configuration section, it is used to tell the CROWler's Engine how to connect to SSLLabs and use its services. SSLLabs will be accessed through CROWler's Detection Rules."
        type: "object"
        properties:
          api_key:
            title: "SSLLabs API Key"
            description: "This is the API key that the CROWler will use to connect to SSLLabs."
            type: "string"
        additionalProperties: "false"
      url_haus:
        title: "URLHaus Configuration"
        description: "This is the URLHaus configuration section, it is used to tell the CROWler's Engine how to connect to URLHaus and use its services. URLHaus will be accessed through CROWler's Detection Rules."
        type: "object"
        properties:
        additionalProperties: "false"
      virus_total:
        title: "VirusTotal Configuration"
        description: "This is the VirusTotal configuration section, it is used to tell the CROWler's Engine how to connect to VirusTotal and use its services. VirusTotal will be accessed through CROWler's Detection Rules."
        type: "object"
        properties:
          api_key:
            title: "VirusTotal API Key"
            description: "This is the API key that the CROWler will use to connect to VirusTotal."
            type: "string"
        additionalProperties: "false"
        required:
        - "api_key"
    additionalProperties: "false"
  os:
    title: "CROWler (internal) Platform OS Configuration"
    description: "This is the operating system that the CROWler will use to run. For example, linux, windows or macos. This field is set automatically by the CROWler itself, so no need to set it manually."
    type: "string"
  debug_level:
    title: "CROWler Debug Level Configuration"
    description: "This is the debug level for the CROWler. It is the level of debugging that the CROWler will use to log messages. The higher the level, the more messages will be logged. Don't set or use 0 for NO debug messages."
    type: "integer"
    examples:
    - "1"
additionalProperties: "false"
oneOf:
- title: "Remote Configuration Mode"
  description: "Configuration where the 'remote' field must be provided. In this case, the local configuration sections like 'database', 'crawler', and others should not be populated."
  allOf:
  - $ref: "#/properties/remote"
  - title: "Remote Configuration Mode requirements"
    type: "object"
    required:
    - "remote"
    not:
      title: "Local Configuration exclusions"
      required:
      - "database"
      - "crawler"
      - "api"
      - "selenium"
      - "network_info"
- title: "Local Configuration Mode"
  description: "Configuration where the 'database', 'crawler', 'api', 'selenium', and 'network_info' fields must be provided. In this case, the 'remote' field should not be populated."
  type: "object"
  properties:
    database:
      $ref: "#/properties/database"
    crawler:
      $ref: "#/properties/crawler"
    api:
      $ref: "#/properties/api"
    selenium:
      $ref: "#/properties/selenium"
    network_info:
      $ref: "#/properties/network_info"
  required:
  - "database"
  - "crawler"
  - "api"
  - "selenium"
  - "network_info"
  not:
    title: "Local Configuration Mode exclusions"
    required:
    - "remote"
dependencies:
  database:
    not:
      required:
      - "remote"
  remote:
    not:
      required:
      - "database"
      - "crawler"
      - "api"
      - "selenium"
      - "network_info"
